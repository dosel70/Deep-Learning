# Deep-Learning

### Deep Learning, 딥 러닝
- 인공 신경망 (Artificial Neural Network)의 층을 연속적으로 깊게 쌓아올려 데이터를 학습하는 방식을 의미한다.
  
- 인간이 학습하고 기억하는 매커니즘을 모방한 기계학습이다.

- 인간은 학습 시, 뇌에 있는 뉴런이 자극을 받아들여서 일정 자극 이상이 되면, 화학물질을 통해 다른 뉴런과 연결되며 해당 부분이 발달한다.

- 자극이 약하거나 기준치를 넘지 못하면, 뉴런은 연결되지 않는다.

- 입력한 데이터가 활성 함수에서 임계점을 넘게 되면 출력된다.

- 초기 인공 신경망(Perceptron)에서 깊게 층을 쌓아 학습하는 딥 러닝으로 발전한다.

- 딥 러닝은 Input nodes layer,Hidden nodes layer , Output nodes layer, 이렇게 세 가지 층이 존재한다.

---

### SLP(Single Layer Perceptron), 단층 퍼셉트론 , 단일 퍼셉트론  

- 가장 단순한 형태의 신경망으로써, Hidden Layer(은닉층)이 없고, Single Layer로 구성되어 있다.

- 퍼셉트론의 구조는 입력 Feature와 가중치, activation function, 출력 값으로 구성되어 있다.

- 신경 세포에서 신호를 전달하는 축삭돌기의 역할을 퍼셉트론에서는 가중치가 대신하고, 입력 값과 가중치 값은 모두 인공 뉴런(활성함수) 으로 도착한다.

- 가중치의 값이 클수록 해당 입력 값이 중요하다는 뜻이고, 인공뉴런(활성함수)에 도착한 각 입력 값과 가중치 값을 곱한 뒤 전체 합한 값을 구한다.

- 인공뉴런 (활성함수)은 보통 시그모이드 함수와 같은 계단 함수를 사용하여, 합한 값을 확률로 반환하고 이때 임계치를 기준으로 0 또는 1을 출력한다.

- 로지스틱 회귀 모델이 인공 신경망에서는 하나의 인공뉴런으로 볼 수 있다.

- 결과적으로 퍼셉트론은 회귀 모델과 마찬가지로 실제 값과 예측 값의 차이가 최소가 되는 가중치 값을 찾는 과정이 퍼셉트론이 학습하는 과정이다.

- 최초 가중치 값을 설정한 뒤 입력 feature로 예측 값을 계산하고, 실제 값과의 차이를 구한 뒤 이를 줄일 수 있도록 가중치 값을 변경한다.

- 퍼셉트론의 활성화 정도를 편향(bias)으로 조절 할 수 있으며, 편향을 통해 어느정도의 자극을 미리 주고 시작 할 수 있다.

- 만약에 뉴런이 활성화 되기 위해 필요한 자극이 1000이라고 가정하면, 입력 값을 500만 받아도 편향을 2로 주어 1000을 만들 수 있다.

### SGD(Stochastic Gradient Descent), 경사하강법  
- 경사 하강법 방식은 전체 학습 데이터를 기반으로 계산한다. 하지만 입력 데이터가 크고 레이어가 많을 수록 자원이 소모된다.
- 
- 일반적으로 메모리 부족으로 인해 연산이 불가능 하기 때문에, 이를 극복하고자 SGD 방식이 도입 되었다.
- 
- 전체 학습 데이터 중, 단 한 건만 임의로 선택하여 경사하강법을 실시하는 방식을 의미한다.
- 
- 많은 건 수 중에 한 건만 실시하기 때문에, 빠르게 최적점을 찾을 수 있지만 노이즈가 심하다.
- 
- 무작위로 추출된 샘플 데이터에 대해 경사 하강법을 실시 하기 때문에, 진폭이 크고 불안정해 보일 수 있다.
- 
- 일반적으로 사용되지 않고 SGD를 얘기할 때에는 보통 미니 배치 경사 하강법을 의미한다.

### Mini-Batch Gradient Descent, 미니 배치 경사 하강법 
- 전체 학습 데이터 중, 특정 크기(Batch 크기)만큼 임의로 선택해서 경사 하강법을 실시한다. 이 또한 확률적 경사 하강법이다.

- 전체 학습 데이터가 1000건이라고 하고, batch size를 100건이라 가정하면, 전체 데이터를 batch size 만큼 나눠서 가져온 뒤 섞고, 경사 하강법을 계산한다.

- 이 경우 10번 반복해야 1000개의 데이터가 모두 학습 되고 이를 epoch라고 한다. 즉 10 epoch * 100 batch이다.

---  

### Multi Layer Perceptron, 다층 퍼셉트론, 다중 퍼셉트론   
- 보다 복잡한 문제의 해결을 위해서 입력층과 출력층 사이에 은닉층이 포함되어 있다.
- 퍼셉트론을 여러층 쌓은 인공 신경망으로서, 각 층에서는 활성 함수를 통해 입력을 처리한다.
- 층이 깊어질수록 정확한 분류가 가능해지지만, 너무 깊어지면 **Overfitting** 이 발생한다.

### ANN (Artificial Neural Network) 인공신경망 
- 은닉층이 1개일 경우, 이를 인공 신경망이라고 한다.

### DNN (Deep Neural Network), 심층 신경망 
- 은닉층이 2개 이상일 경우 이를 심층 신경망이라고 한다.

### Back-propagation , 역전파  
- 심층 신경망에서 최종 출력(예측)을 하기 위한 식이 생기지만 너무 복잡해지기 때문에 편미분을 진행하기에 한계가 있다.
- 즉 편미분을 통해 가중치 값을 구하고, 경사 하강법을 통해 가중치 값을 업데이트 하며, 손실 함수의 최소값을 찾아야 하는데, 순 방향으로는 복잡한 미분식을 계산할 수 없다.
  따라서 미분의 연쇄 법칙 (Chain Rule)을 사용하여 역방향으로 편미분을 진행한다.

---

### Activation Function, 활성화 함수 
- 인공 신경망에서 입력 값에 가중치를 곱한 뒤 합한 결과를 적용하는 함수이다.
---  
#### 1. 시그모이드 함수 (Sigmoid Function)  
- 은닉층이 아닌 최종 활성화 함수, 즉 출력층에서 사용한다.
- 은닉에서 사용 시, 입력 값이 양의 방향으로 큰 경우일 경우, 출력 값의 변화가 없으며, 음의 방향도 마찬가지이다.
  
  평균이 0이 아니기 때문에 정규 분포 형태가 아니고, 방향에 따라 기울기가 달라져서 탐색 경로가 비효율적(지그재그)이 된다.

#### 2. 소프트맥스 함수 (Softmax Function)  
- 은닉층이 아닌 최종 활성화 함수(출력층) 에서 사용한다.
- 시그모이드와 유사하게 0~1 사이의 값을 출력하지만, 이진 분류가 아닌 다중 분류를 통해 모든 확률 값이 1이 되도록 해준다.
- 여러 개의 타겟 데이터를 분류하는 다중 분류의 최종 활성화 함수(출력층)로 사용된다.

#### 3. 탄젠트 함수  (Tangent Function)
- 은닉층이 아닌 최종 활성화함수(출력층)에서 사용된다.
- 은닉층에서 사용 시, 시그모이드와 달리 -1 ~ 1 사이의 값을 출력해서 평균이 0이 될 수 있지만,

     여전히 입력 값의 양의 방향으로 큰 값일 경우 출력 값의 변화가 미비하고 음의 방향도 마찬가지이다.

#### 4. 렐루 함수 (Relu Function)  
- 대표적인 은닉층의 활성 함수이다.
- 입력 값이 0보다 작으면 출력은 0, 0보다 크면 입력값을 출력하게 된다.

### Cross Entropy  
- 실제 데이터의 확률 분포와, 학습된 모델이 계산한 확률 분포의 차이를 구하는 데 사용된다.
- 분류 문제에서 원-핫 인코딩을 통해 사용할 수 있는 오차 계산법이다.

---  
### Optimizer, 최적화  
- 최적의 경사 하강법을 적용하기 위해 필요하며, 최소값을 찾아가는 방법들을 의미한다.
- loss를 줄이는 방향으로 최소 loss를 보다 빠르고 안정적으로 수렴할 수 있어야 한다.

#### 1. Momentum 
- 가중치를 계속 업데이트 할때마다 이전의 값을 일정 수준 반영 시키면서 새로운 가중치로 업데이트 한다.
- 지역 최소값에서 벗어나지 못하는 문제를 해결 할 수 있으며, 진행했던 방향만큼 추가적으로 더하여 관성처럼 빠져나올 수 있게 해준다.

#### 2. Adagrad (Adaptive Gradient)  
- 가중치 별로 서로 다른 학습률을 동적으로 적용한다.
- 적게 변화된 가중치는 보다 큰 학습률을 적용하고, 많이 변화된 가중치는 보다 작은 학습률을 적용 시킨다.
- 처음에는 큰 보폭으로 이동하다가 최소값에 가까워질 수록 작은 보폭으로 이동하게 된다.
- 과거의 모든 기울기를 사용하기 때문에 학습률이 급격히 감소하여, 분모가 커짐으로써 학습률이 0에 가까워지는 문제가 있다.

#### 3. RMSProp(Root Mean Square Propagation)  
- AdaGrad의 단점을 보완한 기법으로써, 학습률이 지나치게 작아지는 것을 막기 위해 **지수 가중 평균법(exponentially weighted average)** 을 통해 구한다.
- 지수 가중 평균법이란, 데이터의 이동 평균을 구할 때 오래된 데이터가 미치는 영향을 지수적으로 감쇠하도록 하는 방법이다.
- 이전의 기울기들을 똑같이 더해가는 것이 아니라, 훨씬 이전의 기울기는 조금 반영하고 최근의 기울기를 많이 반영한다.
- feature마다 적절한 학습률을 적용하여 효율적인 학습을 진행할 수 있고, AdaGrad 보다 학습을 오래 할 수 있다.

#### 4. Adam (Adaptive Moment Estimation)  
- Momentum과 RMSProp 두 가지 방식을 결합한 형태로써, 진행하던 속도에 관성을 주고, 지수 가중 평균법을 적용한 알고리즘이다.
- 최적화 방법 중에서 가장 많이 사용되는 알고리즘이다.


---

### CNN (Convolutional Neural Network), 합성곱 신경망  
- 실제 이미지 데이터는 분류 대상이 이미지에서 고정된 위치에 있지 않고 분류 대상이 고정된 위치에 있지 않은 경우가 대부분이다.
- 실제 이미지 데이터를 분류하기 위해서는, 이미지의 각 Feature들을 그대로 학습하는 것이 아닌, CNN으로 패턴을 인식한 뒤 학습해야 한다.
- 이미지의 크기가 커 질 수록 굉장히 많은 Weight가 필요하기 때문에, 분류기에 바로 넣지 않고 이를 사전에 추출 및 축소 해야 한다.
- CNN은 인간의 시신경 구조를 모방한 기술로써, 이미지의 패턴을 찾을 때 사용한다.
- Feature Extraction을 통해 각 단계를 거치면서, 함축된 이미지 조각으로 분리되고, 각 이미지 조각을 통해 이미지의 패턴을 인식한다.
- CNN은 분류하기에 적합한 최적의 feature를 추출하고, 최적의 feature를 추출하기 위한 최적의 Weight와 Filter를 계산한다.

---

### Filter 
- filter 의 개수 -> 출력 채널의 개수, bias에 반영된다. (필터 안에 커널이 있고, 커널의 개수는 입력 이미지의 채널의 개수와 같다. 커널의 사이즈는 정해져 있지 않음)
- 보통 정방 행렬로 구성되어 있으며, 원본 이미지에 슬라이딩 윈도우 알고리즘을 사용하여 순차적으로 새로운 픽셀 값을 만들면서 적용한다.
- 사용자가 목적에 맞는 특정 필터를 만들거나, 기존에 설계된 다양한 필터를 선택하여 이미지에 적용한다. 하지만 CNN은 최적의 필터(필드)값을 학습하여 스스로 최적화 한다.
- 필터 하나 당, 이미지 채널 수 만큼 kernel이 존재하고, 각 채널에 할당된 필터의 커널을 적용하여 출력 이미지를 생한다.
- 출력 feature map 의 개수는 필터의 개수와 동일하다.

### Kernel  
- filter 안에 1 ~ n 개의 커널이 존재한다. 커널의 개수는 반드시 이미지의 채널 수와 동일해야 한다.
- kernel Size는 가로 * 세로를 의미하며, 가로와 세로는 서로 다를 수 있지만 보통은 일치 시킨다.
- kernel Size가 크면 클 수록, 입력한 이미지에서 더 많은 feature 정보를 가져올 수 있지만,
  큰 사이즈의 Kernel로 Convolution Backbone을 할 경우, 훨씬 더 많은 연산량과 파라미터가 필요하다.

### Stride  
- 입력 이미지에 Convolution Filter를 적용할 때 Sliding Window가 이동하는 간격을 의미한다.
- 기본 stride 는 1이지만, 2를 적용하면 입력 feature map 대비 출력 feature map의 크기가 절반정도 줄어든다. (1/2) 만큼
- stride를 키우면 feature 정보를 손실할 가능성이 높아지지만, 오히려 불필요한 특성을 제거하는 효과를 가져올 수 있고, Convolution 연산 속도를 향상 시킨다.

#### Padding  
- Filter를 적용하여 Convolution 수행 시 출력 feature map이 입력 feature map 대비 계속해서 작아지는 것을 막기 위해 사용한다.
- Filter 적용 전, 입력 feature map의 상하좌우 끝에 각각 열과 행을 추가한 뒤, 0으로 채워서 크기를 증가시킨다.
- 출력 이미지와 크기를 입력 이미지의 크기와 동일하게 유지하기 위해서 직접 계산할 필요 없이 "same"이라는 값을 전달하면 입력 이미지의 크기와 동일하게 맞출 수 있다.


#### Pooling   
- Convolution이 적용된 feature map의 일정 영역 별로 하나의 값을 추출하여 feature map의 사이즈를 줄인다.
- 보통은 Convolution -> Relu activation -> Pooling 순서로 적용한다.
- 비슷한 feature들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화시킬 수 있고,

  feature map의 크기가 줄어들기 때문에, 연산 성능이 향상 된다.
- Max Pooling과 Average Pooling이 있으며, Max Pooling은 중요도가 가장 높은 feature를 추출하고,

  Average Pooling은 전체를 버무려서 추출한다.


#### 🚩 정리 
- Stride를 증가시키는 것과 Pooling을 적용하는 것은 출력 feature map의 크기를 줄이는 데 사용하는 것이다.
- Convolution 연산을 진행하면, feature map의 크기를 줄이면, 위치 변화에 따른 feature의 영향도도 줄어들기 때문에, 과적합(overfitting)을 방지할 수 있는 장점이 있다.

- Pooling의 경우 특정 위치의 feature 값이 손실되는 이슈 등으로 인하여 최근 Advanced CNN에서는 많이 사용되지 않는다.
- Classifier 에서는 Fully Connected Layer의 지나친 연결로 인해 많은 파라미터가 생성되므로 오히려 과적합이 발생할 수 있다.


- #### Dropout을 사용해서 Layer간 연결을 줄일 수 있으며 과적합을 방지할 수 있다.

---

### CNN Performance  
- CNN 모델을 제작 할 때, 다양한 기법을 통해 성능 개선 및 과석합 개선이 가능하다.

#### Weight Initialization, 가중치 초기화  
- 처음에 가중치를 어떻게 줄 것인지를 정하는 방법이며, 처음 가중치를 어떻게 설정하느냐에 따라 모델 성능이 크게 달라질 수 있다.
> 1. 사비에르 글로로트 초기화
>
> - 고정된 표준편차를 사용하지 않고, 이전 층의 노드 수에 맞게 현재 층의 가중치를 초기화 한다.
> - 층 마다 노드 개수를 다르게 설정하더라도, 이에 맞게 가중치가 초기화 되기 때문에 고정된 표준편차를 사용하는 것보다 이상치에 민감하지 않다.
> - 활성화 함서가 ReLU일 경우 층이 지날 수록 활성화 값이 고르지 못하게 되는 문제가 생겨서, 출력 층에서만 사용한다.
>
> 2. 카이밍 히 초기화
> - 고정된 표준편차를 사용하지 않고, 이전 층의 노드 수에 맞게 현재 층의 가중치를 초기화 한다.
> - 층 마다 노드 개수를 다르게 설정하더라도, 이에 맞게 가중치가 초기화 되기 때문에 고정된 표준편차를 사용하는 것보다 이상치에 민감하지 않다.
> - 활성화 함수가 ReLU일때, 추천 하는 초기화 방법으로써, 층이 깊어지더라도 모든 활성값이 고르게 분포된다.
>
#### Batch Nomalization, 배치 정규화   
- 입력 데이터 간에 값의 차이가 발생하면, 가중치의 비중도 달라지기 때문에 , 층을 통과할 수록 편차가 심해진다. (내부 공변량 이동)

  이를 내부 공변량 (Internel Convariant Shift)이라고 한다.

- 가중치의 값의 비중이 달라지면, 특정 가중치에 중점을 두면서 경사 하강법이 진행되기 때문에, 모든 입력값을 표준 정규화 하여 최적의 parameter를 보다 빠르게 학습 할 수 있도록 해야 한다.

- 가중치를 초기화 할때, 민감도를 감소 시키고, 학습 속도를 증가시키며, 모델을 일반화 하기 위해서 사용한다.

- BN은 activation function 앞에 적용하면, Weight 값은 평균이 0, 분산이 1인 상태로 정규 분포가 된다.

- ReLU가 activation으로 적용되면 음수에 해당하는 (절반정도) 부분이 0이 된다.

  이러한 문제를 해결하기 위해서 γ(감마)와 β(베타)를 사용해서 음수부분이 모두 0이 되는 것을 막아준다.

#### Global Average Pooling  

- 이전의 Pooling 들은 면적을 줄이기 위해 사용했지만, Global Average Pooling은 면적을 없애고 채널 수 만큼 값을 나오게 한다.
- feature map의 가로 * 세로의 특정 영역을 Sub smapling 하지 않고, 채널 별로 평균 값을 추출한다.
- feature map의 채널 수가 많을 경우 (보통 512개 이상) 이를 적용하고, 채널 수가 적다면 Flatten을 적용한다.
- Flatten 후에 Classification Dense Layer로 이어지면서 많은 파라미터들로 인한 overfitting 유발 가능성 증대 및 학습 시간 증가로 이어지기 때문에, 맨 마지막 feature map의 채널 수가 크다면 Global Average Pooling을 적용하는 것이 더 나을 수 있다.

#### Weight Regularization (가중치 규제) , Weight Decay (가중치 감소)  
- loss function은 loss 값이 작아지는 방향으로 가중치를 update 한다.
- 하지만, loss를 줄이는 데에만 신경 쓰게 되면, 특정 가중치가 너무 커지면서 오히려 나쁜 결과를 가져올 수 있다.
- 기존 가중치에 특정 연산을 수행하여 loss function의 출력 값과 더해주면 loss function의 결과를 어느정도 제어할 수 있게 된다.
- 보통 파라미터가 많은 Dense Layer에서 많이 사용되고 가중치 규제보다는 loss function 에 규제를 걸어 가중치를 감소 시키는 원리이다. -> 일반화 시킨다.
- kernel_regularizer 파라미터에서 l1, l2를 선택 할 수 있다.

